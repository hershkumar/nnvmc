{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e535440a",
   "metadata": {},
   "source": [
    "# Neural Network Variational Monte Carlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b48d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "os.environ[\"JAX_ENABLE_X64\"]=\"false\"\n",
    "\n",
    "\n",
    "# os.environ[\"XLA_FLAGS\"]=\"--xla_cpu_multi_thread_eigen=true intra_op_parallelism_threads=16 --xla_force_host_platform_device_count={}\".format(multiprocessing.cpu_count())\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_cpu_multi_thread_eigen=true intra_op_parallelism_threads=16\"\n",
    "\n",
    "from jax import numpy as jnp\n",
    "from jax import random, jit, jacfwd, grad, vmap, jvp\n",
    "from jax.nn import one_hot\n",
    "from jax.lax import fori_loop\n",
    "from jax.tree_util import tree_map\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import mc # sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bosonic = True\n",
    "if bosonic:\n",
    "    N = 50\n",
    "else:\n",
    "    N_up = 2\n",
    "    N_down = 1\n",
    "# number of pairwise interactions for the delta function sampling\n",
    "if bosonic: \n",
    "        num_interactions = N * (N - 1) // 2\n",
    "else:\n",
    "    num_interactions = N_up * N_down\n",
    "\n",
    "\n",
    "g = 0 # delta coupling\n",
    "omega = 1\n",
    "m = 1\n",
    "harmonic_omega = 1\n",
    "sigma = -m*harmonic_omega*g/2 # long range coupling\n",
    "C = 20 # denominator constant for the symmetrization\n",
    "\n",
    "G_N_CORES = 16\n",
    "\n",
    "\n",
    "\n",
    "def astra_energy():\n",
    "    return (N * harmonic_omega)/2 - m * g**2  * (N*(N**2 - 1))/(24)\n",
    "\n",
    "true_energy = astra_energy()\n",
    "\n",
    "print(\"True energy: \", true_energy)\n",
    "# compute a 1% bound for the true energy\n",
    "true_energy_lower = true_energy * 0.99\n",
    "true_energy_upper = true_energy * 1.01\n",
    "\n",
    "total_energy = []\n",
    "total_uncerts = []\n",
    "\n",
    "fig = go.FigureWidget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e23e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    features: list[int]  # e.g. [64, 64, 1]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.Dense(feat)(x)\n",
    "            x = nn.celu(x)\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@jit\n",
    "def transform(coords):\n",
    "    x = coords / C                                  # (N,)\n",
    "    powers = jnp.cumprod(jnp.broadcast_to(x, (x.shape[0], x.shape[0])), axis=0)\n",
    "    return jnp.sum(powers, axis=1)\n",
    "\n",
    "def create_model(rng_key, N, features):\n",
    "    model = MLP(features=features)\n",
    "    dummy_x = jnp.ones((N,))          # N inputs of length d â†’ shape (N,d)\n",
    "    params = model.init(rng_key, dummy_x)[\"params\"]\n",
    "    return model, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params = create_model(random.PRNGKey(int(time.time())), N, [50, 100, 200, 200, 100,1])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69561c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def A(x, params):\n",
    "    \"\"\"Neural network output, A(x) given a set of parameters\"\"\"\n",
    "    return model.apply({'params': params}, transform(x)).squeeze() + omega * jnp.sum(x**2.)\n",
    "\n",
    "@jit\n",
    "def psi(x, params):\n",
    "    \"\"\"Wavefunction, psi(x) given a set of parameters\"\"\"\n",
    "    return jnp.exp(-A(x, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcs = mc.Sampler(psi, (N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s, ar = mcs.sample_batched(params, (10**5)//16, 1000, 5, .3, jnp.zeros((N,)), num_chains=16, key=random.PRNGKey(int(time.time())))\n",
    "# print(\"num samples:\", s.shape[0])\n",
    "# print(ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcc294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s, ar = mcs.sample_fast(params, (10**5), 1000, 5, .3, jnp.zeros((N,)), seed=int(time.time()))\n",
    "# print(\"num samples:\", s.shape[0])\n",
    "# print(ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchains = 16\n",
    "pos_initials = jnp.zeros((nchains, N))\n",
    "seeds = jnp.arange(nchains) + int(time.time())\n",
    "samples, acc = mcs.run_many_chains(params, (10**5)//G_N_CORES, 1000, 5, 0.15, pos_initials, seeds)\n",
    "print(\"num samples:\", samples.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility functions for pytrees\n",
    "\n",
    "# multiplies a pytree by a scalar elementwise\n",
    "def pytree_scalar_mult(scalar, pytree):\n",
    "    return tree_map(lambda x: scalar * x, pytree)\n",
    "\n",
    "# adds two pytrees elementwise\n",
    "def pytree_add(a, b):\n",
    "    return tree_map(lambda x, y: x + y, a, b)\n",
    "\n",
    "# returns a pytree whos elements are the mean over the first axis of the input pytree elements\n",
    "def tree_mean(pytree_batched):\n",
    "    return tree_map(lambda x: jnp.mean(x, axis=0), pytree_batched)\n",
    "\n",
    "# generates an empty pytree with the same structure as the input pytree\n",
    "def tree_zeros_like(pytree):\n",
    "    return tree_map(jnp.zeros_like, pytree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the wavefunction with respect to the parameters\n",
    "dnn_dtheta = jit(grad(psi, 1))\n",
    "vdnn_dtheta = jit(vmap(dnn_dtheta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "grad_psi_x = jit(grad(psi, 0))  # returns (N,)\n",
    "@partial(jit, static_argnames=(\"N\",))\n",
    "def laplacian_psi_jvp(x, params, *, N):\n",
    "    def g(x_):\n",
    "        return grad_psi_x(x_, params)\n",
    "\n",
    "    def body(i, acc):\n",
    "        ei = one_hot(i, N, dtype=x.dtype)\n",
    "        _, dg = jvp(g, (x,), (ei,))\n",
    "        return acc + dg[i]\n",
    "\n",
    "    return fori_loop(0, N, body, jnp.array(0.0, x.dtype))\n",
    "\n",
    "\n",
    "dA_dtheta = jit(grad(A, 1))\n",
    "vdA_dtheta = vmap(dA_dtheta, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "dA_dx = jit(grad(A, 0))\n",
    "vdA_dx = vmap(dA_dx, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "A_hessian = jacfwd(jit(grad(A, 0)), 0)\n",
    "@jit\n",
    "def d2A_dx2(coords, params):\n",
    "    return jnp.diag(A_hessian(coords, params))\n",
    "\n",
    "\n",
    "@jit\n",
    "def sigma_potential(x):\n",
    "    # pairwise |xi-xj|\n",
    "    diffs = jnp.abs(x[:, None] - x[None, :])         # (N,N)\n",
    "    return sigma * jnp.sum(jnp.triu(diffs, k=1))     # sum_{i<j}\n",
    "\n",
    "\n",
    "@jit\n",
    "def Es_nodelta(coords, params):\n",
    "    return - (1/2) * (1/ psi(coords, params)) * laplacian_psi_jvp(coords, params, N=N) + (1/2) * jnp.sum(coords**2) + sigma_potential(coords)\n",
    "\n",
    "vEs_nodelta = vmap(Es_nodelta, in_axes=(0,None), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def Es_delta(coords, coords_prime, params, alpha, g):\n",
    "    return num_interactions * g * (psi(coords_prime, params)**2)/(psi(coords, params)**2) * (1/(jnp.sqrt(jnp.pi)*alpha))*jnp.exp(-(coords[-1]/alpha)**2)\n",
    "\n",
    "vEs_delta = vmap(Es_delta, in_axes=(0,0, None, None, None), out_axes=0)\n",
    "\n",
    "\n",
    "@jit\n",
    "def gradient_comp(coords, coords_prime, params, es_nodelta, energy_calc, es_delta):\n",
    "    return pytree_add(pytree_scalar_mult(2/psi(coords, params) * (es_nodelta - energy_calc), dnn_dtheta(coords, params)), pytree_scalar_mult(2 * es_delta / psi(coords_prime, params), dnn_dtheta(coords_prime, params)))\n",
    "\n",
    "vgradient_comp = vmap(gradient_comp, in_axes=(0, 0, None, 0, None, 0), out_axes=0)\n",
    "\n",
    "\n",
    "\n",
    "@partial(jit, static_argnames=(\"chunk_size\",\"g\"))\n",
    "def compute_stats_and_grad(samples, samples_prime, params, g, *, chunk_size=256):\n",
    "    n = samples.shape[0]\n",
    "\n",
    "    # alpha\n",
    "    ys = samples[:, -1]\n",
    "    maxabs = jnp.max(jnp.abs(ys))\n",
    "    alpha = maxabs / jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 1e-10))\n",
    "\n",
    "    # energies\n",
    "    e0 = vEs_nodelta(samples, params)\n",
    "    ed = vEs_delta(samples, samples_prime, params, alpha, g)\n",
    "    e = e0 + ed\n",
    "\n",
    "    mean_e = jnp.mean(e)\n",
    "    var_e = jnp.mean(e * e) - mean_e * mean_e\n",
    "    uncert = jnp.sqrt(jnp.maximum(var_e, 0.0)) / jnp.sqrt(n)\n",
    "\n",
    "    # chunks\n",
    "    nb = (n + chunk_size - 1) // chunk_size\n",
    "\n",
    "    grad0 = tree_zeros_like(dnn_dtheta(samples[0], params))\n",
    "\n",
    "    def body(i, grad_sum):\n",
    "        start = i * chunk_size\n",
    "        idx = start + jnp.arange(chunk_size)          # (chunk_size,)\n",
    "        mask = (idx < n).astype(samples.dtype)        # (chunk_size,)\n",
    "\n",
    "        idx = jnp.minimum(idx, n - 1)                 # clamp for safe gather\n",
    "\n",
    "        s_chunk  = samples[idx, :]\n",
    "        sp_chunk = samples_prime[idx, :]\n",
    "        e0_chunk = e0[idx]\n",
    "        ed_chunk = ed[idx]\n",
    "\n",
    "        grads = vgradient_comp(s_chunk, sp_chunk, params, e0_chunk, mean_e, ed_chunk)\n",
    "\n",
    "        # multiply each leaf by mask on leading axis, then sum over axis 0\n",
    "        def masked_sum(x):\n",
    "            # x has leading dimension chunk_size\n",
    "            reshape = (chunk_size,) + (1,) * (x.ndim - 1)\n",
    "            return jnp.sum(x * mask.reshape(reshape), axis=0)\n",
    "\n",
    "        grads_sum = tree_map(masked_sum, grads)\n",
    "        return pytree_add(grad_sum, grads_sum)\n",
    "\n",
    "    grad_sum = fori_loop(0, nb, body, grad0)\n",
    "    mean_grad = pytree_scalar_mult(1.0 / n, grad_sum)\n",
    "\n",
    "    return mean_grad, mean_e, uncert\n",
    "\n",
    "#TODO: Now that the sampling is jitted, we can jit this function as well\n",
    "def gradient(params, g, num_samples=10**3, thermal=200, skip=5, variation_size=1.0, sampling=\"serial\", chunk_size=256):\n",
    "    if sampling == \"serial\":\n",
    "        samples, _ = mcs.sample_fast(params, num_samples, thermal, skip, variation_size, jnp.zeros((N,)), int(time.time()))\n",
    "    elif sampling == \"parallel\":\n",
    "        seeds = jnp.arange(G_N_CORES) + int(time.time())\n",
    "        init_positions = jnp.zeros((G_N_CORES, N))\n",
    "        samples, _ = mcs.run_many_chains(params, num_samples//G_N_CORES, thermal, skip, variation_size, init_positions, seeds )\n",
    "\n",
    "    samples_prime = mcs.sample_prime(samples)\n",
    "    return compute_stats_and_grad(samples, samples_prime, params, g, chunk_size=chunk_size)\n",
    "\n",
    "\n",
    "def step(params, opt_state, step_num, num_samples, thermal, skip, variation_size, g, sampling, chunk_size):\n",
    "    \"\"\"\n",
    "    One optimization step.\n",
    "    - params: current parameters pytree\n",
    "    - opt_state: optimizer state (must be carried across steps)\n",
    "    Returns: (new_params, new_opt_state, energy, uncert)\n",
    "    \"\"\"\n",
    "    grad, energy, uncert = gradient(\n",
    "        params,\n",
    "        g,\n",
    "        num_samples=num_samples,\n",
    "        thermal=thermal,\n",
    "        skip=skip,\n",
    "        variation_size=variation_size,\n",
    "        sampling=sampling,\n",
    "        chunk_size=chunk_size,\n",
    "    )\n",
    "    new_opt_state = opt_update(step_num, grad, opt_state)\n",
    "    new_params = get_params(new_opt_state)\n",
    "\n",
    "    return new_params, new_opt_state, energy, uncert\n",
    "\n",
    "def update_plot(total_energy, total_uncerts):\n",
    "    with fig.batch_update():\n",
    "        fig.data[0].x = np.arange(len(total_energy))\n",
    "        fig.data[0].y = total_energy\n",
    "        fig.data[0].error_y.array = total_uncerts\n",
    "\n",
    "        n = len(total_energy)\n",
    "        for i in [1, 2, 3]:\n",
    "            fig.data[i].x = [0, n]\n",
    "              \n",
    "\n",
    "def train(params, iterations, num_samples, thermal, skip, variation_size, g, sampling=\"serial\", chunk_size=256, energy_storage=total_energy, uncert_storage=total_uncerts):\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    Returns: (hs, us, ns, final_params)\n",
    "    \"\"\"\n",
    "    hs, us = [], []\n",
    "    ns = np.arange(iterations)\n",
    "\n",
    "    # Initialize optimizer state ONCE\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "    old_params = params\n",
    "\n",
    "    for step_num in pbar:\n",
    "        new_params, opt_state, energy, uncert = step(\n",
    "            old_params,\n",
    "            opt_state,\n",
    "            step_num,\n",
    "            num_samples,\n",
    "            thermal,\n",
    "            skip,\n",
    "            variation_size,\n",
    "            g,\n",
    "            sampling=sampling,\n",
    "            chunk_size=chunk_size,\n",
    "        )\n",
    "        \n",
    "\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params\n",
    "\n",
    "        energy_storage.append(energy)\n",
    "        uncert_storage.append(uncert)\n",
    "        update_plot(energy_storage, uncert_storage)\n",
    "\n",
    "        pbar.set_description(f\"Energy = {energy}\", refresh=True)\n",
    "\n",
    "        # Use jnp.isnan if energy is a JAX scalar; np.isnan is OK if it's a Python float\n",
    "        if np.isnan(np.asarray(energy)):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            #TODO: backtrack to previous params, try again?\n",
    "            break\n",
    "\n",
    "    return hs, us, ns, old_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9183a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "energy_trace = fig.add_scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    error_y=dict(type='data', array=[], visible=True),\n",
    "    mode='lines+markers',\n",
    "    name='Energy vs Iteration'\n",
    ")\n",
    "\n",
    "\n",
    "# add a horizontal line for the true energy\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, len(total_energy)],\n",
    "    y=[true_energy, true_energy],\n",
    "    mode='lines',\n",
    "    name='True Energy',\n",
    "    line=dict(dash='dash', color='red')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, len(total_energy)],\n",
    "    y=[true_energy_lower, true_energy_lower],\n",
    "    mode='lines',\n",
    "    name='True Energy Lower Bound',\n",
    "    line=dict(dash='dot', color='green')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, len(total_energy)],\n",
    "    y=[true_energy_upper, true_energy_upper],\n",
    "    mode='lines',\n",
    "    name='True Energy Upper Bound',\n",
    "    line=dict(dash='dot', color='green')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='VMC Energy Convergence',\n",
    "    xaxis_title='Iteration',\n",
    "    yaxis_title='Energy',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10 ** (-3))\n",
    "# params, N_steps, N_samples, thermalization, skip, variation_size, g, sampling_type, gradient chunk size\n",
    "resultsa = train(params, 100, 100000, 1000, 2, 0.15, g, sampling=\"parallel\", chunk_size=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10 ** (-4))\n",
    "\n",
    "resultsb = train(resultsa[3], 100, 100000, 1000, 4, 0.15, g, sampling=\"parallel\", chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efd046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnvmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
